{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************************************************************************************\n",
    "# Author: Andi Sama \n",
    "# Purpose: Face Liveness Detection\n",
    "#   - Illustrates detection of human face liveness through blinking eyes\n",
    "#     * Source: Video stream (Webcam/Video file)\n",
    "#     * Process: Capture image in video, convert to grey then pass to a trained model for face recognition\n",
    "#         & eyes recognition. Model (recognize eyes) is trained using keras (based on tensorflow)\n",
    "#     * Output: Trained Face Recognized, Blinking Eyes Detected (open->close->open)\n",
    "# Organization: Sinergi Wahana Gemilang\n",
    "# Creation Date: April 1, 2020\n",
    "# Changes history:\n",
    "#   - April 6-10, 2020: Solving compatibility issues \n",
    "#       * convert scipy resize and reshape functions to numpy arrays (asm_eye_status.ipynb)\n",
    "#   - April 10, 2020: Basic things done\n",
    "#       * blinking eyes detection works, use a pre-trained model (94% accuracy)\n",
    "#       * 'p' keystroke to pause while cv2 is showing frames with overlays\n",
    "#   - April 11: Add additional video input \n",
    "#       * add a video file as input stream, in init() function\n",
    "#   - April 12: Face recognition by name, using a trained model\n",
    "#       * add 2 Korean artists in Netflix: \"Chief of Staff\" movie\n",
    "#   - April 14-16: preparing for article in medium\n",
    "#   - April 18-19: finalized\n",
    "#       * User Selection for input source (Webcam/Default video file)\n",
    "#       * Add section in this notebook to retrain keras model to recognize eyes \n",
    "#         Pass 'number of of max epoch' to train & experiment with training/validation accuracies\n",
    "#   - April 24-26: Exception handling, write processed video to file\n",
    "#       * exception handling for empty frame in detect_and_display()\n",
    "#       * write processed video (overlay w/ face recognition/blinking eyes detection) to a file\n",
    "#       * add date & time overlay as header in detect_and_display()\n",
    "#       * add image overlay (eye clipart) if blinking eye detected in detect_and_display()\n",
    "#   - May 2: Final - exception handling, write processed video to file\n",
    "#       * fix logic error in processing select_source() return code (str -> int)\n",
    "#       * test existing code with camera input: face with mask (OK)\n",
    "# References:\n",
    "#   A fork from https://github.com/Guarouba/face_rec (2019)\n",
    "#   See article in medium.com/@andisama\n",
    "# *************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS name: nt , system: Windows , release: 10\n",
      "Anaconda version:\n",
      "# packages in environment at C:\\Users\\andis\\anaconda3:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_anaconda_depends         2019.03                  py37_0  \n",
      "anaconda                  custom                   py37_1  \n",
      "anaconda-client           1.7.2                    py37_0  \n",
      "anaconda-navigator        1.9.12                   py37_0  \n",
      "anaconda-project          0.8.4                      py_0  \n",
      "Python version: 3.7.7 (default, Mar 23 2020, 23:19:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Python version info:  sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\n",
      "OpenCV version: 4.2.0\n",
      "numpy version: 1.18.1\n",
      "Keras, tensorflow version: 2.2.4-tf 2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Recognition version: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "import os, platform, sys, time\n",
    "from datetime import date\n",
    "print('OS name:', os.name, ', system:', platform.system(), ', release:', platform.release())\n",
    "print(\"Anaconda version:\")\n",
    "!conda list anaconda\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Python version info: \", sys.version_info)\n",
    "import cv2\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "import numpy as np\n",
    "print(\"numpy version:\", np.__version__)\n",
    "import tensorflow as tf\n",
    "print(\"Keras, tensorflow version:\", tf.keras.__version__, tf.__version__)\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from asm_eye_status import * \n",
    "import face_recognition\n",
    "print(\"Face Recognition version:\", face_recognition.__version__)\n",
    "import imutils\n",
    "from imutils.video import VideoStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a deep learning model to recognize open/closed eyes\n",
    "# # April 18-19, 2020\n",
    "# #   => asm_eye_status.ipynb \n",
    "# # The following needs to be done only once\n",
    "# #  - Train the deep learning model to recognize open & closed eyes\n",
    "# #  - Once the model has been generated, this whole cell can be marked all as comments\n",
    "# #  - 2 model files will be generated in current directory\n",
    "# #    * model.h5 (keras-based model)\n",
    "# #    * model.json\n",
    "# # in about 100 epochs, achieved quite a good combination of training & validation accuracy\n",
    "# #    * Epoch 100/100\n",
    "# #    * 118/118 - 2s 19ms/step - loss: 0.0104 - accuracy: 0.9979 - val_loss: 5.0244e-06 - val_accuracy: 0.9556\n",
    "# # (in general, 20 epochs should be enough to achieve about 94-95% accuracy on training & validation)\n",
    "# epoch = 100\n",
    "# # collect images dataset from dataset/train (training) and dataset/val (validation) directories\n",
    "# train_generator, val_generator = collect()\n",
    "# # train & save the model (h5 and json file)\n",
    "# train(train_generator, val_generator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(video_source):\n",
    "    face_cascPath = 'haarcascade_frontalface_alt.xml'\n",
    "    # face_cascPath = 'lbpcascade_frontalface.xml'\n",
    "\n",
    "    open_eye_cascPath = 'haarcascade_eye_tree_eyeglasses.xml'\n",
    "    left_eye_cascPath = 'haarcascade_lefteye_2splits.xml'\n",
    "    right_eye_cascPath ='haarcascade_righteye_2splits.xml'\n",
    "    dataset = 'faces'\n",
    "\n",
    "    face_detector = cv2.CascadeClassifier(face_cascPath)\n",
    "    open_eyes_detector = cv2.CascadeClassifier(open_eye_cascPath)\n",
    "    left_eye_detector = cv2.CascadeClassifier(left_eye_cascPath)\n",
    "    right_eye_detector = cv2.CascadeClassifier(right_eye_cascPath)\n",
    "\n",
    "    # asama: modified to include input stream from a video file\n",
    "    # run one of the following... input from video file or from integrated camera\n",
    "    # 1. Either this - Integrated Camera\n",
    "    source_resolution = (0, 0)\n",
    "    \n",
    "    if video_source == 0:\n",
    "        print(\"[LOG] Opening webcam...\")\n",
    "        \n",
    "        # video_capture = cv2.VideoCapture(0) # if using this one: OpenCV VideoCapture() => very slow \n",
    "\n",
    "        print(\"[LOG] Getting Camera Resolution...\")\n",
    "        # use this cv2.VideoCapture() just to get resolution of camera \n",
    "        cam = cv2.VideoCapture(0)\n",
    "        cam_width = int(cam.get(3))\n",
    "        cam_height = int(cam.get(4))\n",
    "        source_resolution = (cam_width, cam_height) \n",
    "        print('Camera resolution (width, height) in pixels:', source_resolution)\n",
    "        cam.release()  # immediately release camera after getting the resolution\n",
    "        \n",
    "        # switch to imutils VideoStream() for better bufferred frames' reading from camera\n",
    "        video_capture = VideoStream(src=0).start() # imutils VideoStream(), much faster\n",
    "\n",
    "    # 2. Or this one - 2nd: video file (video_source other than 0)\n",
    "    else:\n",
    "        current_directory = os.getcwd()\n",
    "        video_file = 'data\\Chief of Staff 2 Ep 1 Trailer.mp4'\n",
    "        print(\"[LOG] Opening default video file...\", current_directory + video_file)\n",
    "\n",
    "        # video_capture = cv2.VideoCapture(video_file) # this one using OpenCV VideoCapture() is very slow \n",
    "\n",
    "        print(\"[LOG] Getting Video Resolution...\")\n",
    "        # use this cv2.VideoCapture() just to get resolution of the video file\n",
    "        cam = cv2.VideoCapture(video_file)\n",
    "        cam_width = int(cam.get(3))\n",
    "        cam_height = int(cam.get(4))\n",
    "        source_resolution = (cam_width, cam_height) \n",
    "        print('Video resolution (width, height) in pixels:', source_resolution)\n",
    "        cam.release()  # immediately release camera after getting the resolution\n",
    "        \n",
    "        video_capture = VideoStream(src=video_file).start()  # imutils VideoStream(), much faster\n",
    "#         source_resolution = ()\n",
    "#         print('Video  resolution (width, height) in pixels:', source_resolution)\n",
    "        \n",
    "    model = load_model()\n",
    "\n",
    "    print(\"[LOG] Collecting images...\")\n",
    "    images = []\n",
    "    for direc, _, files in tqdm(os.walk(dataset)):\n",
    "        for file in files:\n",
    "            if file.endswith(\"jpg\"):\n",
    "                images.append(os.path.join(direc,file))\n",
    "    # print(\" >DEBUG (collected file names):\", images)\n",
    "    return (model, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, \\\n",
    "            video_capture, images, source_resolution) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_encode(images):\n",
    "    # initialize the list of known encodings and known names\n",
    "    known_encodings = []\n",
    "    known_names = []\n",
    "    print(\"[LOG] Encoding faces...\")\n",
    "\n",
    "    for image_path in tqdm(images):\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Convert it from BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "     \n",
    "        # detect face in the image and get its location (square boxes coordinates)\n",
    "        boxes = face_recognition.face_locations(image, model='hog')\n",
    "\n",
    "        # Encode the face into a 128-d embeddings vector\n",
    "        encoding = face_recognition.face_encodings(image, boxes)\n",
    "\n",
    "        # the person's name is the name of the folder where the image comes from\n",
    "        name = image_path.split(os.path.sep)[-2]\n",
    "\n",
    "        if len(encoding) > 0 : \n",
    "            known_encodings.append(encoding[0])\n",
    "            known_names.append(name)\n",
    "\n",
    "    return {\"encodings\": known_encodings, \"names\": known_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBlinking(history, maxFrames):\n",
    "    \"\"\" @history: A string containing the history of eyes status \n",
    "         where a '1' means that the eyes were closed and '0' open.\n",
    "        @maxFrames: The maximal number of successive frames where an eye is closed \"\"\"\n",
    "    for i in range(maxFrames):\n",
    "        pattern = '1' + '0'*(i+1) + '1'\n",
    "        if pattern in history:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_display(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected, source_resolution, img_overlay):\n",
    "        #  ret, frame = video_capture.read() # OpenCV version, very slow\n",
    "        frame = video_capture.read() # imutils VideoStream version, much faster\n",
    "\n",
    "        # video frame resize        \n",
    "#         # OpenCV version, very slow\n",
    "#         if ret == True:\n",
    "#             frame = cv2.resize(frame, (0, 0), fx=1.0, fy=1.0)\n",
    "#             # frame = cv2.resize(frame, (0, 0), fx=0.6, fy=0.6)\n",
    "#         else:\n",
    "#             print('error reading - camera problem or file error?, exiting...')\n",
    "#             return None\n",
    "\n",
    "        # imutils VideoStream version for read buffering, much faster\n",
    "        if frame is None:\n",
    "            print('empty frame detected! - camera closed or end of file?, exiting...')\n",
    "            return frame\n",
    "        else:\n",
    "            frame = cv2.resize(frame, (0, 0), fx=1.0, fy=1.0)\n",
    "            # frame = cv2.resize(frame, (0, 0), fx=0.6, fy=0.\n",
    "\n",
    "        frame = cv2.flip(frame, 1) # flip horizontal\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_detector.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(50, 50),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "\n",
    "        # for each detected face\n",
    "        for (x,y,w,h) in faces:\n",
    "            # Encode the face into a 128-d embeddings vector\n",
    "            encoding = face_recognition.face_encodings(rgb, [(y, x+w, y+h, x)])[0]\n",
    "\n",
    "            # Compare the vector with all known faces encodings\n",
    "            matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "\n",
    "            # For now we don't know the person name\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # If there is at least one match:\n",
    "            if True in matches:\n",
    "                matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "                counts = {}\n",
    "                for i in matchedIdxs:\n",
    "                    name = data[\"names\"][i]\n",
    "                    counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "                # determine the recognized face with the largest number of votes\n",
    "                name = max(counts, key=counts.get)\n",
    "\n",
    "            face = frame[y:y+h,x:x+w]\n",
    "            gray_face = gray[y:y+h,x:x+w]\n",
    "\n",
    "            eyes = []\n",
    "            \n",
    "            # Eyes detection\n",
    "            # check first if eyes are open (with glasses taking into account)\n",
    "            open_eyes_glasses = open_eyes_detector.detectMultiScale(\n",
    "                gray_face,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30),\n",
    "                flags = cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "            # if open_eyes_glasses detect eyes then they are open \n",
    "            if len(open_eyes_glasses) == 2:\n",
    "                eyes_detected[name]+='1'\n",
    "                for (ex,ey,ew,eh) in open_eyes_glasses:\n",
    "                    cv2.rectangle(face,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "            \n",
    "            # otherwise try detecting eyes using left and right_eye_detector\n",
    "            # which can detect open and closed eyes                \n",
    "            else:\n",
    "                # separate the face into left and right sides\n",
    "                left_face = frame[y:y+h, x+int(w/2):x+w]\n",
    "                left_face_gray = gray[y:y+h, x+int(w/2):x+w]\n",
    "\n",
    "                right_face = frame[y:y+h, x:x+int(w/2)]\n",
    "                right_face_gray = gray[y:y+h, x:x+int(w/2)]\n",
    "\n",
    "                # Detect the left eye\n",
    "                left_eye = left_eye_detector.detectMultiScale(\n",
    "                    left_face_gray,\n",
    "                    scaleFactor=1.1,\n",
    "                    minNeighbors=5,\n",
    "                    minSize=(30, 30),\n",
    "                    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "                )\n",
    "\n",
    "                # Detect the right eye\n",
    "                right_eye = right_eye_detector.detectMultiScale(\n",
    "                    right_face_gray,\n",
    "                    scaleFactor=1.1,\n",
    "                    minNeighbors=5,\n",
    "                    minSize=(30, 30),\n",
    "                    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "                )\n",
    "\n",
    "                eye_status = '1' # we suppose the eyes are open\n",
    "\n",
    "                # For each eye check wether the eye is closed.\n",
    "                # If one is closed we conclude the eyes are closed\n",
    "                for (ex,ey,ew,eh) in right_eye:\n",
    "                    color = (0,255,0)\n",
    "                    pred = predict(right_face[ey:ey+eh,ex:ex+ew],model)\n",
    "                    if pred == 'closed':\n",
    "                        eye_status='0'\n",
    "                        color = (0,0,255)\n",
    "                    cv2.rectangle(right_face,(ex,ey),(ex+ew,ey+eh),color,2)\n",
    "                for (ex,ey,ew,eh) in left_eye:\n",
    "                    color = (0,255,0)\n",
    "                    pred = predict(left_face[ey:ey+eh,ex:ex+ew],model)\n",
    "                    if pred == 'closed':\n",
    "                        eye_status='0'\n",
    "                        color = (0,0,255)\n",
    "                    cv2.rectangle(left_face,(ex,ey),(ex+ew,ey+eh),color,2)\n",
    "                eyes_detected[name] += eye_status\n",
    "\n",
    "            # current date & time \n",
    "            c_datetime = str(date.today()) + ' ' + time.strftime(\"%H:%M:%S\")\n",
    "            width = source_resolution[0]\n",
    "            height = source_resolution[1]\n",
    "            x_hdr = int(width * 0.01) # starting bottom_x is 1% of the width on the top left\n",
    "            y_hdr = int(height * 0.1) # startgin bottom_y is 10% of the height on the top left\n",
    "            cv2.putText(frame, c_datetime, (x_hdr, y_hdr), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)\n",
    "\n",
    "            # location to put image overlay \"blinking eye\", maintaining 1% distance from top right x & y\n",
    "            # blinking eyes image resolution to be displayed is 100x40 (width x height)\n",
    "            x2_hdr = int(width - (.01 * width) - 100) # starting bottom_x     \n",
    "            y2_hdr = int(height - (.99 * height)) # starting bottom_y  \n",
    "            \n",
    "            # Each time, we check if the person has blinked\n",
    "            # If yes, we display its name\n",
    "            if isBlinking(eyes_detected[name],3):\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                # Display name\n",
    "                y = y - 15 if y - 15 > 15 else y + 15\n",
    "                cv2.putText(frame, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "\n",
    "                # display image overlay\n",
    "                alpha = 0.4\n",
    "                added_image = cv2.addWeighted(frame[y2_hdr:y2_hdr+40,x2_hdr:x2_hdr+100,:],alpha, img_overlay[0:40,0:100,:],1-alpha,0)\n",
    "                # Change the region with the result\n",
    "                frame[y2_hdr:y2_hdr+40,x2_hdr:x2_hdr+100] = added_image\n",
    "       \n",
    "        return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_source():\n",
    "    valid_selections = ('0', '1')\n",
    "    prompt = \"Please select source:\\n \\\n",
    "        0: Webcam\\n \\\n",
    "        1: Videofile\\n\"\n",
    "    selection = input(prompt)\n",
    "    while not(selection in valid_selections):\n",
    "        selection = input(prompt)\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Initialization...\n",
      "Please select source:\n",
      "         0: Webcam\n",
      "         1: Videofile\n",
      "1\n",
      "[LOG] Opening default video file... C:\\Users\\andis\\Code\\FaceRecdata\\Chief of Staff 2 Ep 1 Trailer.mp4\n",
      "[LOG] Getting Video Resolution...\n",
      "Video resolution (width, height) in pixels: (1280, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 1335.77it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Collecting images...\n",
      "[LOG] Encoding faces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:47<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Detecting & Showing Images...\n",
      "empty frame detected! - camera closed or end of file?, exiting...\n",
      "[LOG] Writing output file... output/video_face-blink_detect.mp4\n",
      "[LOG] All done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"[LOG] Initialization...\")\n",
    "\n",
    "    # input in init(video_source); 0:WebCam, 1:VideoFile\n",
    "    video_source = int(select_source())\n",
    "    (model, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, \\\n",
    "         video_capture, images, source_resolution) = init(video_source)\n",
    "    data = process_and_encode(images)\n",
    "\n",
    "    # overlay image (eye clipart) width: 100, height: 40\n",
    "    img_overlay = cv2.imread('data/icon_eye_100x40.png')\n",
    "    \n",
    "    # Define output filename\n",
    "    out_dir = 'output/'\n",
    "    if video_source == 0: # camera\n",
    "        out_filename = out_dir + 'camera_face-blink_detect.mp4'\n",
    "    else: # video file\n",
    "        out_filename = out_dir + 'video_face-blink_detect.mp4'\n",
    "        \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    frame_rate = 5\n",
    "    out = cv2.VideoWriter(out_filename, fourcc, frame_rate, source_resolution)\n",
    "    \n",
    "    eyes_detected = defaultdict(str)\n",
    "    imshow_label = \"Face Liveness Detector - Blinking Eyes (q-quit, p-pause)\"\n",
    "    print(\"[LOG] Detecting & Showing Images...\")\n",
    "\n",
    "    while True:\n",
    "        frame = detect_and_display(model, video_capture, face_detector, open_eyes_detector,left_eye_detector,right_eye_detector, data, eyes_detected, source_resolution, img_overlay)\n",
    "        if frame is None:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        cv2.imshow(imshow_label, frame)\n",
    "        \n",
    "        # asama: modified to include p=pause\n",
    "        key_pressed = cv2.waitKey(1)\n",
    "        if key_pressed & 0xFF == ord('q'): # q=quit\n",
    "            break\n",
    "        elif key_pressed & 0xFF == ord('p'): # p=pause\n",
    "            cv2.waitKey(-1)\n",
    "\n",
    "    print(\"[LOG] Writing output file...\", out_filename)            \n",
    "    video_capture.stop()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"[LOG] All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
